{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fd867b-fa4a-4da1-b474-95914ebdca6b",
   "metadata": {},
   "source": [
    "### Data Preparation:\n",
    "- Unfiltered, Shuffled Distance Measurement Data\n",
    "- Unfiltered, Shuffled Angle Measurement Data\n",
    "- Unfiltered, Shuffled RMSD Measurement Data\n",
    "- Unfiltered, UnShuffled Distance Measurement Data\n",
    "- Filtered, Shuffled Distance Measurement Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf1d14-33ff-41f5-9c06-75e7c5420f97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Unfiltered Distance Measurement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d5869-71a7-43a8-8465-12b8cf4a245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define the range of window sizes\n",
    "window_range = range(2, 52)\n",
    "\n",
    "def import_lcc_data(lccdata_folder):\n",
    "    \"\"\"\n",
    "    Imports Local Compaction data files (.csv) and assigns them to a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - lccdata_folder: The folder where Local Compaction data files are stored.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with window sizes as keys and pandas DataFrames as values.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    for window_size in window_range:\n",
    "        file_path = os.path.join(lccdata_folder, f\"WT_Simulation_WS_{window_size}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            df.index.name = None \n",
    "            data_dict[window_size] = df\n",
    "        else:\n",
    "            print(f\"Warning: File not found for Window Size {window_size}: {file_path}\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "# Folder containing the Local Compaction data\n",
    "lccdata_folder = 'Local_Compaction/Local_Compaction_Data'\n",
    "\n",
    "# Import LCC data for the wild-type protein\n",
    "wt_dict = import_lcc_data(lccdata_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf26ff4-383d-4be5-93c4-f7be8cca4f98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Unfiltered, Shuffled Distance Measurement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0b8c6-6d54-4236-97c6-70680f4be03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder):\n",
    "    \"\"\"\n",
    "    Save the datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    X_train.to_csv(os.path.join(folder_name, 'X_train_f.csv'))\n",
    "    y_train.to_csv(os.path.join(folder_name, 'y_train_f.csv'))\n",
    "    X_valid.to_csv(os.path.join(folder_name, 'X_valid_f.csv'))\n",
    "    y_valid.to_csv(os.path.join(folder_name, 'y_valid_f.csv'))\n",
    "\n",
    "def load_datasets(fold, base_folder):\n",
    "    \"\"\"\n",
    "    Load datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    \n",
    "    X_train = pd.read_csv(os.path.join(folder_name, 'X_train_f.csv'), index_col=0)\n",
    "    y_train = pd.read_csv(os.path.join(folder_name, 'y_train_f.csv'), index_col=0)\n",
    "    X_valid = pd.read_csv(os.path.join(folder_name, 'X_valid_f.csv'), index_col=0)\n",
    "    y_valid = pd.read_csv(os.path.join(folder_name, 'y_valid_f.csv'), index_col=0)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def normalize_to_range_0_1(data):\n",
    "    \"\"\"\n",
    "    Normalize data to range [0, 1] and round to 6 decimal places.\n",
    "    \"\"\"\n",
    "    normalized = (data - data.min()) / (data.max() - data.min())\n",
    "    return normalized.round(6)\n",
    "\n",
    "def preprocessing_kfold(wt_data, n_splits=5, base_folder=\"AE_Data/Unfiltered_Shuffled_Compaction_Data\"):\n",
    "    \"\"\"\n",
    "    Preprocess data using KFold cross-validation, normalize it, and save it.\n",
    "    \"\"\"\n",
    "    # Ensure all data is numeric\n",
    "    wt_data = wt_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Shuffle the DataFrame rows without resetting the index\n",
    "    wt_data = wt_data.sample(frac=1, random_state=42)\n",
    "    \n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold = 1\n",
    "    for train_index, valid_index in kf.split(wt_data):\n",
    "        folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "        if not os.path.exists(os.path.join(folder_name, 'X_train_f.csv')):\n",
    "            X_train, X_valid = wt_data.iloc[train_index], wt_data.iloc[valid_index]\n",
    "            y_train = pd.DataFrame({'class': [0] * len(X_train)}, index=X_train.index)\n",
    "            y_valid = pd.DataFrame({'class': [0] * len(X_valid)}, index=X_valid.index)\n",
    "\n",
    "            # Normalize training and validation data to [0, 1] and round to 6 decimal places\n",
    "            X_train = normalize_to_range_0_1(X_train)\n",
    "            X_valid = normalize_to_range_0_1(X_valid)\n",
    "\n",
    "            print(f\"Fold {fold}:\")\n",
    "            print(f\"Training set shape: {X_train.shape}\")\n",
    "            print(f\"Validation set shape: {X_valid.shape}\")\n",
    "            \n",
    "            save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder)\n",
    "        else:\n",
    "            print(f\"Data for fold {fold} already exists, skipping generation.\")\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca7c21-3978-4be1-a982-eaa3fb85b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window sizes to be used\n",
    "window_sizes = list(range(2, 52))\n",
    "\n",
    "# Concatenate all DataFrames in wt_dict into a single DataFrame\n",
    "wt_combined = pd.concat(wt_dict.values(), axis=1)\n",
    "\n",
    "# Preprocess and save data\n",
    "preprocessing_kfold(wt_combined, n_splits=5)\n",
    "\n",
    "# Example of loading a specific fold (e.g., fold 1)\n",
    "base_folder = \"AE_Data/Unfiltered_Shuffled_Compaction_Data\"\n",
    "X_train_loaded, y_train_loaded, X_valid_loaded, y_valid_loaded = load_datasets(fold=1, base_folder=base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c6b09a-be4f-43af-871b-5089f9ad5127",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Unfiltered, UnShuffled Distance Measurement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed301b-83f0-4409-a11b-c9cc1ee8cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder):\n",
    "    \"\"\"\n",
    "    Save the datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    X_train.to_csv(os.path.join(folder_name, 'X_train_f.csv'))\n",
    "    y_train.to_csv(os.path.join(folder_name, 'y_train_f.csv'))\n",
    "    X_valid.to_csv(os.path.join(folder_name, 'X_valid_f.csv'))\n",
    "    y_valid.to_csv(os.path.join(folder_name, 'y_valid_f.csv'))\n",
    "\n",
    "def load_datasets(fold, base_folder):\n",
    "    \"\"\"\n",
    "    Load datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    \n",
    "    X_train = pd.read_csv(os.path.join(folder_name, 'X_train_f.csv'), index_col=0)\n",
    "    y_train = pd.read_csv(os.path.join(folder_name, 'y_train_f.csv'), index_col=0)\n",
    "    X_valid = pd.read_csv(os.path.join(folder_name, 'X_valid_f.csv'), index_col=0)\n",
    "    y_valid = pd.read_csv(os.path.join(folder_name, 'y_valid_f.csv'), index_col=0)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def normalize_to_range_0_1(data):\n",
    "    \"\"\"\n",
    "    Normalize data to range [0, 1] and round to 6 decimal places.\n",
    "    \"\"\"\n",
    "    normalized = (data - data.min()) / (data.max() - data.min())\n",
    "    return normalized.round(6)\n",
    "\n",
    "def preprocessing_kfold(wt_data, n_splits=5, base_folder=\"AE_Data/Unfiltered_UnShuffled_Compaction_Data\"):\n",
    "    \"\"\"\n",
    "    Preprocess data using KFold cross-validation, normalize it, and save it without shuffling.\n",
    "    \"\"\"\n",
    "    # Ensure all data is numeric\n",
    "    wt_data = wt_data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)  # Do not shuffle\n",
    "    fold = 1\n",
    "    for train_index, valid_index in kf.split(wt_data):\n",
    "        folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "        if not os.path.exists(os.path.join(folder_name, 'X_train_f.csv')):\n",
    "            X_train, X_valid = wt_data.iloc[train_index], wt_data.iloc[valid_index]\n",
    "            y_train = pd.DataFrame({'class': [0] * len(X_train)}, index=X_train.index)\n",
    "            y_valid = pd.DataFrame({'class': [0] * len(X_valid)}, index=X_valid.index)\n",
    "\n",
    "            # Normalize training and validation data to [0, 1] and round to 6 decimal places\n",
    "            X_train = normalize_to_range_0_1(X_train)\n",
    "            X_valid = normalize_to_range_0_1(X_valid)\n",
    "\n",
    "            print(f\"Fold {fold}:\")\n",
    "            print(f\"Training set shape: {X_train.shape}\")\n",
    "            print(f\"Validation set shape: {X_valid.shape}\")\n",
    "            \n",
    "            save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder)\n",
    "        else:\n",
    "            print(f\"Data for fold {fold} already exists, skipping generation.\")\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050af55-84f5-440d-903b-3bd61cb24f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window sizes to be used\n",
    "window_sizes = list(range(2, 52))\n",
    "\n",
    "# Concatenate all DataFrames in wt_dict into a single DataFrame\n",
    "wt_combined = pd.concat(wt_dict.values(), axis=1)\n",
    "\n",
    "# Preprocess and save data\n",
    "preprocessing_kfold(wt_combined, n_splits=5)\n",
    "\n",
    "# Example of loading a specific fold (e.g., fold 1)\n",
    "base_folder = \"AE_Data/Unfiltered_UnShuffled_Compaction_Data\"\n",
    "X_train_loaded, y_train_loaded, X_valid_loaded, y_valid_loaded = load_datasets(fold=1, base_folder=base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6633a5-42f8-4e61-9ed2-cac55869a236",
   "metadata": {},
   "source": [
    "# Filtered, Shuffled Distance Measurement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881df355-2b2d-48da-b955-1663c932072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define the range of window sizes\n",
    "window_range = range(2, 49)\n",
    "\n",
    "def import_lcc_data(lccdata_folder):\n",
    "    \"\"\"\n",
    "    Imports Local Compaction data files (.csv) and assigns them to a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - lccdata_folder: The folder where Local Compaction data files are stored.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with window sizes as keys and pandas DataFrames as values.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    for window_size in window_range:\n",
    "        file_path = os.path.join(lccdata_folder, f\"WT_{window_size}_f.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            df.index.name = None \n",
    "            data_dict[window_size] = df\n",
    "        else:\n",
    "            print(f\"Warning: File not found for Window Size {window_size}: {file_path}\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "# Folder containing the Local Compaction data\n",
    "lccdata_folder = 'XGB_High_vs_Low_Energy/Filtered_Local_Compaction_Data'\n",
    "\n",
    "# Import LCC data for the wild-type protein\n",
    "wt_dict = import_lcc_data(lccdata_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be455873-340a-4c60-9c49-77a91f427042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder):\n",
    "    \"\"\"\n",
    "    Save the datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    X_train.to_csv(os.path.join(folder_name, 'X_train_f.csv'))\n",
    "    y_train.to_csv(os.path.join(folder_name, 'y_train_f.csv'))\n",
    "    X_valid.to_csv(os.path.join(folder_name, 'X_valid_f.csv'))\n",
    "    y_valid.to_csv(os.path.join(folder_name, 'y_valid_f.csv'))\n",
    "\n",
    "def load_datasets(fold, base_folder):\n",
    "    \"\"\"\n",
    "    Load datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    \n",
    "    X_train = pd.read_csv(os.path.join(folder_name, 'X_train_f.csv'), index_col=0)\n",
    "    y_train = pd.read_csv(os.path.join(folder_name, 'y_train_f.csv'), index_col=0)\n",
    "    X_valid = pd.read_csv(os.path.join(folder_name, 'X_valid_f.csv'), index_col=0)\n",
    "    y_valid = pd.read_csv(os.path.join(folder_name, 'y_valid_f.csv'), index_col=0)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def normalize_to_range_0_1(data):\n",
    "    \"\"\"\n",
    "    Normalize data to range [0, 1] and round to 6 decimal places.\n",
    "    \"\"\"\n",
    "    normalized = (data - data.min()) / (data.max() - data.min())\n",
    "    return normalized.round(6)\n",
    "\n",
    "def preprocessing_kfold(wt_data, n_splits=5, base_folder=\"AE_Data/Filtered_Shuffled_Compaction_Data\"):\n",
    "    \"\"\"\n",
    "    Preprocess data using KFold cross-validation, normalize it, and save it.\n",
    "    \"\"\"\n",
    "    # Ensure all data is numeric\n",
    "    wt_data = wt_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Shuffle the DataFrame rows without resetting the index\n",
    "    wt_data = wt_data.sample(frac=1, random_state=42)\n",
    "    \n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold = 1\n",
    "    for train_index, valid_index in kf.split(wt_data):\n",
    "        folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "        if not os.path.exists(os.path.join(folder_name, 'X_train_f.csv')):\n",
    "            X_train, X_valid = wt_data.iloc[train_index], wt_data.iloc[valid_index]\n",
    "            y_train = pd.DataFrame({'class': [0] * len(X_train)}, index=X_train.index)\n",
    "            y_valid = pd.DataFrame({'class': [0] * len(X_valid)}, index=X_valid.index)\n",
    "\n",
    "            # Normalize training and validation data to [0, 1] and round to 6 decimal places\n",
    "            X_train = normalize_to_range_0_1(X_train)\n",
    "            X_valid = normalize_to_range_0_1(X_valid)\n",
    "\n",
    "            print(f\"Fold {fold}:\")\n",
    "            print(f\"Training set shape: {X_train.shape}\")\n",
    "            print(f\"Validation set shape: {X_valid.shape}\")\n",
    "            \n",
    "            save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder)\n",
    "        else:\n",
    "            print(f\"Data for fold {fold} already exists, skipping generation.\")\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a79d1-538d-495d-8dc5-8b9060a6bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window sizes to be used\n",
    "window_sizes = list(range(2, 52))\n",
    "\n",
    "# Concatenate all DataFrames in wt_dict into a single DataFrame\n",
    "wt_combined = pd.concat(wt_dict.values(), axis=1)\n",
    "\n",
    "# Preprocess and save data\n",
    "preprocessing_kfold(wt_combined, n_splits=5)\n",
    "\n",
    "# Example of loading a specific fold (e.g., fold 1)\n",
    "base_folder = \"AE_Data/Filtered_Shuffled_Compaction_Data\"\n",
    "X_train_loaded, y_train_loaded, X_valid_loaded, y_valid_loaded = load_datasets(fold=1, base_folder=base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36610f-957f-4236-a90e-55007dd81fde",
   "metadata": {},
   "source": [
    "# Unfiltered, Shuffled Angle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05227252-0d72-4985-b409-665222c14da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder):\n",
    "    \"\"\"\n",
    "    Save the datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    X_train.to_csv(os.path.join(folder_name, 'X_train_f.csv'))\n",
    "    y_train.to_csv(os.path.join(folder_name, 'y_train_f.csv'))\n",
    "    X_valid.to_csv(os.path.join(folder_name, 'X_valid_f.csv'))\n",
    "    y_valid.to_csv(os.path.join(folder_name, 'y_valid_f.csv'))\n",
    "\n",
    "def load_datasets(fold, base_folder):\n",
    "    \"\"\"\n",
    "    Load datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    \n",
    "    X_train = pd.read_csv(os.path.join(folder_name, 'X_train_f.csv'), index_col=0)\n",
    "    y_train = pd.read_csv(os.path.join(folder_name, 'y_train_f.csv'), index_col=0)\n",
    "    X_valid = pd.read_csv(os.path.join(folder_name, 'X_valid_f.csv'), index_col=0)\n",
    "    y_valid = pd.read_csv(os.path.join(folder_name, 'y_valid_f.csv'), index_col=0)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def normalize_to_range_0_1(data):\n",
    "    \"\"\"\n",
    "    Normalize data to range [0, 1] and round to 6 decimal places.\n",
    "    \"\"\"\n",
    "    normalized = (data - data.min()) / (data.max() - data.min())\n",
    "    return normalized.round(6)\n",
    "\n",
    "def load_angle_data(angle_folder, angle_window_sizes):\n",
    "    \"\"\"\n",
    "    Load angle data for all window sizes and combine into a single DataFrame.\n",
    "    \"\"\"\n",
    "    angle_data = {}\n",
    "    for window_size in angle_window_sizes:\n",
    "        file_path = os.path.join(angle_folder, f'Angles_WS_{window_size}.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            angle_data[window_size] = pd.read_csv(file_path, index_col=0)\n",
    "        else:\n",
    "            print(f\"Warning: Angle data file not found for window size {window_size}.\")\n",
    "    return angle_data\n",
    "\n",
    "def combine_angle_data_only(angle_data, angle_window_sizes):\n",
    "    \"\"\"\n",
    "    Combine all angle data into a single DataFrame, sorted by window size.\n",
    "    \"\"\"\n",
    "    # Start with the index of one DataFrame (assuming all indices are consistent)\n",
    "    first_window_size = list(angle_data.keys())[0]\n",
    "    combined_data = pd.DataFrame(index=angle_data[first_window_size].index)\n",
    "\n",
    "    for window_size in angle_window_sizes:\n",
    "        if window_size in angle_data:\n",
    "            combined_data = pd.concat([combined_data, angle_data[window_size]], axis=1)\n",
    "        else:\n",
    "            print(f\"Warning: Missing angle data for window size {window_size}. Skipping.\")\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "def preprocessing_kfold(angle_data_only, n_splits=5, base_folder=\"AE_Data/Unfiltered_Shuffled_Angle_Data\"):\n",
    "    \"\"\"\n",
    "    Preprocess angle data using KFold cross-validation, normalize it, and save it.\n",
    "    \"\"\"\n",
    "    # Ensure all data is numeric\n",
    "    angle_data_only = angle_data_only.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Shuffle the DataFrame rows without resetting the index\n",
    "    angle_data_only = angle_data_only.sample(frac=1, random_state=42)\n",
    "    \n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold = 1\n",
    "    for train_index, valid_index in kf.split(angle_data_only):\n",
    "        folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "        if not os.path.exists(os.path.join(folder_name, 'X_train_f.csv')):\n",
    "            X_train, X_valid = angle_data_only.iloc[train_index], angle_data_only.iloc[valid_index]\n",
    "            y_train = pd.DataFrame({'class': [0] * len(X_train)}, index=X_train.index)\n",
    "            y_valid = pd.DataFrame({'class': [0] * len(X_valid)}, index=X_valid.index)\n",
    "\n",
    "            # Normalize training and validation data to [0, 1] and round to 6 decimal places\n",
    "            X_train = normalize_to_range_0_1(X_train)\n",
    "            X_valid = normalize_to_range_0_1(X_valid)\n",
    "\n",
    "            print(f\"Fold {fold}:\")\n",
    "            print(f\"Training set shape: {X_train.shape}\")\n",
    "            print(f\"Validation set shape: {X_valid.shape}\")\n",
    "            \n",
    "            save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder)\n",
    "        else:\n",
    "            print(f\"Data for fold {fold} already exists, skipping generation.\")\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4818-1602-470a-92cf-d444cbc5c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and window sizes\n",
    "angle_window_sizes = list(range(1, 51))\n",
    "angle_folder = \"Local_Angles/Angle_Data\"\n",
    "\n",
    "# Load and combine angle data only\n",
    "angle_data = load_angle_data(angle_folder, angle_window_sizes)\n",
    "combined_angle_data = combine_angle_data_only(angle_data, angle_window_sizes)\n",
    "\n",
    "# Preprocess and save data\n",
    "preprocessing_kfold(combined_angle_data, n_splits=5, base_folder=\"AE_Data/Unfiltered_Shuffled_Angle_Data\")\n",
    "\n",
    "# Example of loading a specific fold (e.g., fold 1)\n",
    "base_folder = \"AE_Data/Unfiltered_Shuffled_Angle_Data\"\n",
    "X_train_loaded, y_train_loaded, X_valid_loaded, y_valid_loaded = load_datasets(fold=1, base_folder=base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e986e02-7b15-4d94-bf46-55d4e12868ff",
   "metadata": {},
   "source": [
    "# Unfiltered, Shuffled, RMSD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf79cc9-79e4-43c9-9835-c8e6de86e910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88676831-0964-4f1b-a5b9-21f5f68c6bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7afb81ae-67cc-4b18-9c9a-766d6b2dbba6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Unfiltered, Shuffled Angle and Distance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd2481-077e-4ed5-8501-4935e7f87c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder):\n",
    "    \"\"\"\n",
    "    Save the datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    X_train.to_csv(os.path.join(folder_name, 'X_train_f.csv'))\n",
    "    y_train.to_csv(os.path.join(folder_name, 'y_train_f.csv'))\n",
    "    X_valid.to_csv(os.path.join(folder_name, 'X_valid_f.csv'))\n",
    "    y_valid.to_csv(os.path.join(folder_name, 'y_valid_f.csv'))\n",
    "\n",
    "def load_datasets(fold, base_folder):\n",
    "    \"\"\"\n",
    "    Load datasets for a specific fold.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "    \n",
    "    X_train = pd.read_csv(os.path.join(folder_name, 'X_train_f.csv'), index_col=0)\n",
    "    y_train = pd.read_csv(os.path.join(folder_name, 'y_train_f.csv'), index_col=0)\n",
    "    X_valid = pd.read_csv(os.path.join(folder_name, 'X_valid_f.csv'), index_col=0)\n",
    "    y_valid = pd.read_csv(os.path.join(folder_name, 'y_valid_f.csv'), index_col=0)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def normalize_to_range_0_1(data):\n",
    "    \"\"\"\n",
    "    Normalize data to range [0, 1] and round to 6 decimal places.\n",
    "    \"\"\"\n",
    "    normalized = (data - data.min()) / (data.max() - data.min())\n",
    "    return normalized.round(6)\n",
    "\n",
    "def load_angle_data(angle_folder, window_sizes):\n",
    "    \"\"\"\n",
    "    Load angle data for all window sizes and combine into a single DataFrame.\n",
    "    \"\"\"\n",
    "    angle_data = {}\n",
    "    for window_size in window_sizes:\n",
    "        file_path = os.path.join(angle_folder, f'Angles_WS_{window_size}.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            angle_data[window_size] = pd.read_csv(file_path, index_col=0)\n",
    "        else:\n",
    "            print(f\"Warning: Angle data file not found for window size {window_size}.\")\n",
    "    return angle_data\n",
    "\n",
    "def combine_distance_and_angle(distance_data, angle_data, distance_window_sizes, angle_window_sizes):\n",
    "    \"\"\"\n",
    "    Combine distance and angle data alternately by window size, starting with distance (window size 2).\n",
    "    \"\"\"\n",
    "    # Start with the index of one DataFrame (assuming all indices are consistent)\n",
    "    first_window_size = list(distance_data.keys())[0]\n",
    "    combined_data = pd.DataFrame(index=distance_data[first_window_size].index)\n",
    "\n",
    "    # Interleave distance and angle window sizes\n",
    "    max_window_size = max(max(distance_window_sizes), max(angle_window_sizes))\n",
    "    for i in range(1, max_window_size + 1):\n",
    "        if i in distance_window_sizes and i in distance_data:\n",
    "            combined_data = pd.concat([combined_data, distance_data[i]], axis=1)\n",
    "        if i in angle_window_sizes and i in angle_data:\n",
    "            combined_data = pd.concat([combined_data, angle_data[i]], axis=1)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing_kfold(combined_data, n_splits=5, base_folder=\"AE_Data/Unfiltered_Shuffled_Compaction_Angle_Data\"):\n",
    "    \"\"\"\n",
    "    Preprocess data using KFold cross-validation, normalize it, and save it.\n",
    "    \"\"\"\n",
    "    # Ensure all data is numeric\n",
    "    combined_data = combined_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Shuffle the DataFrame rows without resetting the index\n",
    "    combined_data = combined_data.sample(frac=1, random_state=42)\n",
    "    \n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold = 1\n",
    "    for train_index, valid_index in kf.split(combined_data):\n",
    "        folder_name = os.path.join(base_folder, f'Training_Set_{fold}')\n",
    "        if not os.path.exists(os.path.join(folder_name, 'X_train_f.csv')):\n",
    "            X_train, X_valid = combined_data.iloc[train_index], combined_data.iloc[valid_index]\n",
    "            y_train = pd.DataFrame({'class': [0] * len(X_train)}, index=X_train.index)\n",
    "            y_valid = pd.DataFrame({'class': [0] * len(X_valid)}, index=X_valid.index)\n",
    "\n",
    "            # Normalize training and validation data to [0, 1] and round to 6 decimal places\n",
    "            X_train = normalize_to_range_0_1(X_train)\n",
    "            X_valid = normalize_to_range_0_1(X_valid)\n",
    "\n",
    "            print(f\"Fold {fold}:\")\n",
    "            print(f\"Training set shape: {X_train.shape}\")\n",
    "            print(f\"Validation set shape: {X_valid.shape}\")\n",
    "            \n",
    "            save_datasets(X_train, y_train, X_valid, y_valid, fold, base_folder)\n",
    "        else:\n",
    "            print(f\"Data for fold {fold} already exists, skipping generation.\")\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80ca9e-2749-4a04-ac98-3b4443597f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and window sizes\n",
    "distance_window_sizes = list(range(2, 52))\n",
    "angle_window_sizes = list(range(1, 51))\n",
    "\n",
    "distance_data = {ws: pd.read_csv(f\"Local_Compaction/Local_Compaction_Data/WT_Simulation_WS_{ws}.csv\", index_col=0) for ws in distance_window_sizes}\n",
    "angle_folder = \"Local_Angles/Angle_Data\"\n",
    "angle_data = load_angle_data(angle_folder, angle_window_sizes)\n",
    "\n",
    "# Combine distance and angle data alternately\n",
    "combined_data = combine_distance_and_angle(distance_data, angle_data, distance_window_sizes, angle_window_sizes)\n",
    "\n",
    "# Preprocess and save data\n",
    "preprocessing_kfold(combined_data, n_splits=5, base_folder=\"AE_Data/Unfiltered_Shuffled_Compaction_Angle_Data\")\n",
    "\n",
    "# Example of loading a specific fold (e.g., fold 1)\n",
    "base_folder = \"AE_Data/Unfiltered_Shuffled_Compaction_Angle_Data\"\n",
    "X_train_loaded, y_train_loaded, X_valid_loaded, y_valid_loaded = load_datasets(fold=1, base_folder=base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514edfbe-73d0-4352-b2b1-324e3464f553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
